{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.16","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":11662703,"sourceType":"datasetVersion","datasetId":7319222},{"sourceId":11666016,"sourceType":"datasetVersion","datasetId":7321580}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:45.865178Z","iopub.execute_input":"2025-05-08T16:41:45.865525Z","iopub.status.idle":"2025-05-08T16:41:45.883913Z","shell.execute_reply.started":"2025-05-08T16:41:45.865499Z","shell.execute_reply":"2025-05-08T16:41:45.878529Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/indaba-test/submission_indabax.csv\n/kaggle/input/indabax-tunisia-2025-anomaly-solver-challenge-2/SampleSubmission.csv\n/kaggle/input/indabax-tunisia-2025-anomaly-solver-challenge-2/SecondChallengeStarterNotebook.ipynb\n/kaggle/input/indabax-tunisia-2025-anomaly-solver-challenge-2/Train.csv\n/kaggle/input/indabax-tunisia-2025-anomaly-solver-challenge-2/Test.csv\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"!pip install rouge_score ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:45.886308Z","iopub.execute_input":"2025-05-08T16:41:45.886538Z","iopub.status.idle":"2025-05-08T16:41:49.420740Z","shell.execute_reply.started":"2025-05-08T16:41:45.886516Z","shell.execute_reply":"2025-05-08T16:41:49.416461Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: rouge_score in /usr/local/lib/python3.10/site-packages (0.1.2)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/site-packages (from rouge_score) (2.0.2)\nRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/site-packages (from rouge_score) (1.17.0)\nRequirement already satisfied: absl-py in /usr/local/lib/python3.10/site-packages (from rouge_score) (2.2.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.10/site-packages (from rouge_score) (3.9.1)\nRequirement already satisfied: click in /usr/local/lib/python3.10/site-packages (from nltk->rouge_score) (8.1.8)\nRequirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/site-packages (from nltk->rouge_score) (2024.11.6)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.10/site-packages (from nltk->rouge_score) (4.67.1)\nRequirement already satisfied: joblib in /usr/local/lib/python3.10/site-packages (from nltk->rouge_score) (1.4.2)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.1.1\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Import necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom transformers import (\n    BartForConditionalGeneration, \n    BartTokenizer, \n    get_linear_schedule_with_warmup\n)\nfrom torch.optim import AdamW\nfrom datasets import Dataset\nimport torch\nfrom torch.utils.data import DataLoader\nfrom rouge_score import rouge_scorer\nfrom tqdm import tqdm\nimport warnings\nfrom transformers import get_scheduler\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:49.422735Z","iopub.execute_input":"2025-05-08T16:41:49.423015Z","iopub.status.idle":"2025-05-08T16:41:49.492212Z","shell.execute_reply.started":"2025-05-08T16:41:49.422982Z","shell.execute_reply":"2025-05-08T16:41:49.487621Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 12\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      7\u001b[0m     BartForConditionalGeneration, \n\u001b[1;32m      8\u001b[0m     BartTokenizer, \n\u001b[1;32m      9\u001b[0m     get_linear_schedule_with_warmup\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AdamW\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dataset\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataLoader\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'"],"ename":"ModuleNotFoundError","evalue":"No module named 'datasets'","output_type":"error"}],"execution_count":6},{"cell_type":"code","source":"# Suppress warnings\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:49.494255Z","iopub.status.idle":"2025-05-08T16:41:49.496080Z","shell.execute_reply.started":"2025-05-08T16:41:49.494772Z","shell.execute_reply":"2025-05-08T16:41:49.494788Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Constants\nMODEL_NAME = \"facebook/bart-base\"\nBATCH_SIZE = 8\nMAX_INPUT_LENGTH = 512\nMAX_TARGET_LENGTH = 128\nNUM_EPOCHS = 4\nLEARNING_RATE = 5e-4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:49.498389Z","iopub.status.idle":"2025-05-08T16:41:49.499465Z","shell.execute_reply.started":"2025-05-08T16:41:49.498561Z","shell.execute_reply":"2025-05-08T16:41:49.498576Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Data Preparation Functions\ndef load_and_preprocess_data(train_path):\n    \"\"\"Load and preprocess the training data.\"\"\"\n    df = pd.read_csv(train_path)\n    \n    # Drop columns with more than 50% missing values\n    threshold = 0.5\n    missing_ratio = df.isnull().mean()\n    cols_to_drop = missing_ratio[missing_ratio > threshold].index\n    df.drop(columns=cols_to_drop, inplace=True)\n    \n    # Fill remaining NaNs with appropriate values\n    for col in df.columns:\n        if df[col].dtype == 'object':\n            # Fill text/object columns with 'unknown'\n            df[col].fillna(\"unknown\", inplace=True)\n        else:\n            # Fill numerical columns with median (more robust than mean for skewed distributions)\n            if df[col].notna().any():  # Only calculate median if there are non-null values\n                median_value = df[col].median()\n                df[col].fillna(median_value, inplace=True)\n            else:\n                # If entire column is NA, fill with 0 as fallback\n                df[col].fillna(0, inplace=True)\n    \n    print(f\"Dropped columns: {list(cols_to_drop)}\")\n    print(f\"Remaining columns: {df.shape[1]}\")\n    return df\n\ndef create_prompts(df, target_column=\"improvement_solutions\"):\n    \"\"\"\n    Create structured and contextual prompts for a telecom assistant model.\n    The model will receive KPIs and a description of the problem, and is expected\n    to suggest a technical solution and corrective action.\n    \"\"\"\n    \n    def row_to_prompt(row):\n        prompt = (\n            \"You are a telecom network assistant. Based on the following network KPIs \"\n            \"and the identified issue, suggest a technical solution and a corrective action.\\n\\n\"\n        )\n        prompt += \"Network KPIs and context:\\n\"\n        \n        for col in df.columns:\n            if col != target_column:\n                value = row[col]\n                if pd.notna(value) and str(value).strip():\n                    prompt += f\"- {col}: {value}\\n\"\n        \n        prompt += \"\\nProblem-solving task:\\n\"\n        prompt += \"What is the root cause, and what solution or corrective action would you recommend?\"\n        \n        return prompt\n\n    df[\"prompt\"] = df.apply(row_to_prompt, axis=1)\n    return df\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:49.500862Z","iopub.status.idle":"2025-05-08T16:41:49.502120Z","shell.execute_reply.started":"2025-05-08T16:41:49.501019Z","shell.execute_reply":"2025-05-08T16:41:49.501032Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Tokenization and Dataset Preparation\ndef prepare_datasets(train_df, val_df, tokenizer):\n    \"\"\"Prepare train and validation datasets.\"\"\"\n    train_dataset = Dataset.from_pandas(train_df[[\"prompt\", \"improvement_solutions\"]])\n    val_dataset = Dataset.from_pandas(val_df[[\"prompt\", \"improvement_solutions\"]])\n    \n    def preprocess_function(examples):\n        model_inputs = tokenizer(\n            examples[\"prompt\"],\n            max_length=MAX_INPUT_LENGTH,\n            truncation=True,\n            padding=\"max_length\"\n        )\n        with tokenizer.as_target_tokenizer():\n            labels = tokenizer(\n                examples[\"improvement_solutions\"],\n                max_length=MAX_TARGET_LENGTH,\n                truncation=True,\n                padding=\"max_length\"\n            )\n        model_inputs[\"labels\"] = labels[\"input_ids\"]\n        return model_inputs\n    \n    tokenized_train = train_dataset.map(preprocess_function, batched=True)\n    tokenized_val = val_dataset.map(preprocess_function, batched=True)\n    \n    tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    tokenized_val.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n    \n    return tokenized_train, tokenized_val\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:49.504429Z","iopub.status.idle":"2025-05-08T16:41:49.506077Z","shell.execute_reply.started":"2025-05-08T16:41:49.504860Z","shell.execute_reply":"2025-05-08T16:41:49.504875Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from torch.nn import CrossEntropyLoss\n\n# Training Functions\ndef initialize_model_and_optimizer():\n    \"\"\"Initialize model, tokenizer, and optimizer.\"\"\"\n    tokenizer = BartTokenizer.from_pretrained(MODEL_NAME)\n    model = BartForConditionalGeneration.from_pretrained(MODEL_NAME)\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n    num_training_steps = len(train_dataloader) * NUM_EPOCHS\n    lr_scheduler = get_scheduler(\n        name=\"linear\", optimizer=optimizer, num_warmup_steps=200, num_training_steps=num_training_steps\n    )\n    return model, tokenizer, optimizer,lr_scheduler\n\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train_model(model, train_dataloader, val_dataloader, optimizer,lr_scheduler, tokenizer, device):\n    \"\"\"Train the model with evaluation and mixed precision.\"\"\"\n    scaler = GradScaler()\n    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n    \n    for epoch in range(NUM_EPOCHS):\n        # --- Training Phase ---\n        model.train()\n        epoch_loss = 0\n        batch_iter = tqdm(train_dataloader, desc=f\"Train Epoch {epoch+1}/{NUM_EPOCHS}\", leave=False)\n        \n        for batch in batch_iter:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            batch['labels'] = torch.clamp(batch['labels'], 0, tokenizer.vocab_size - 1)\n            \n            optimizer.zero_grad()\n            loss_fct = CrossEntropyLoss(label_smoothing=0.1, ignore_index=tokenizer.pad_token_id)\n            with autocast():  # AMP enabled forward pass\n                outputs = model(**batch)\n                logits = outputs.logits\n                loss = loss_fct(logits.view(-1, logits.size(-1)), batch[\"labels\"].view(-1))            \n            scaler.scale(loss).backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            scaler.step(optimizer)\n            lr_scheduler.step()\n            scaler.update()\n            \n            epoch_loss += loss.item()\n            batch_iter.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n        \n        # --- Validation Phase ---\n        model.eval()\n        val_loss = 0\n        rouge_accum = {k: [] for k in ['rouge1', 'rouge2', 'rougeL']}\n        \n        with torch.no_grad():\n            val_iter = tqdm(val_dataloader, desc=f\"Validating Epoch {epoch+1}\", leave=False)\n            for batch in val_iter:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                with autocast():  # Mixed precision inference (optional, safe)\n                    outputs = model(**batch)\n                    val_loss += outputs.loss.item()\n                    \n                    preds = model.generate(\n                        input_ids=batch['input_ids'],\n                        attention_mask=batch['attention_mask'],\n                        max_length=MAX_TARGET_LENGTH,\n                        num_beams=4,\n                        length_penalty=1.2  # encourages longer outputs\n\n                    )\n                \n                decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n                decoded_labels = tokenizer.batch_decode(batch['labels'], skip_special_tokens=True)\n                \n                for pred, label in zip(decoded_preds, decoded_labels):\n                    scores = scorer.score(label, pred)\n                    for k in rouge_accum:\n                        rouge_accum[k].append(scores[k].fmeasure)\n\n        # --- Epoch Summary ---\n        print(f\"\\nEpoch {epoch+1} Summary:\")\n        print(f\"Train Loss: {epoch_loss / len(train_dataloader):.4f}\")\n        print(f\"Val Loss: {val_loss / len(val_dataloader):.4f}\")\n        print(\"ROUGE Scores:\")\n        for k, scores in rouge_accum.items():\n            print(f\"- {k.upper()}: {np.mean(scores):.4f}\" if scores else f\"- {k.upper()}: N/A\")\n        print(\"=\"*50)\n        torch.cuda.empty_cache()\n\n    return model\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:49.507109Z","iopub.status.idle":"2025-05-08T16:41:49.509636Z","shell.execute_reply.started":"2025-05-08T16:41:49.507257Z","shell.execute_reply":"2025-05-08T16:41:49.507270Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader\nfrom datasets import Dataset\n\n#test dataset preparation\n\ndef prepare_test_dataframe(kpi_path, label_path, id_column='ID'):\n    # Load datasets\n    test_problems = pd.read_csv(label_path)\n    test_df = pd.read_csv(kpi_path)\n\n    # Melt the problems dataframe\n    melted_problems = test_problems.melt(\n        id_vars=[id_column],\n        var_name='problem',\n        value_name='present'\n    )\n\n    # Filter present problems\n    present_problems = melted_problems[melted_problems['present'] == 1]\n\n    # Group problems\n    combined_problems = present_problems.groupby(id_column)['problem'].agg(','.join).reset_index()\n\n    # Merge with KPI data\n    test_df = test_df.merge(combined_problems, how='left', on=id_column)\n    test_df = test_df.rename(columns={'problem': 'network_labels'})\n    test_df['network_labels'] = test_df['network_labels'].fillna('normal')\n    test_df.fillna(\"unknown\", inplace=True)\n\n    return test_df\n\ndef create_prompt_column(df):\n    def row_to_prompt(row):\n        return \"; \".join([f\"{col}: {row[col]}\" for col in row.index])\n    df[\"prompt\"] = df.apply(row_to_prompt, axis=1)\n    return df\n\ndef tokenize_dataset(df, tokenizer):\n    dataset = Dataset.from_pandas(df[[\"prompt\"]])\n\n    def tokenize_fn(examples):\n        return tokenizer(\n            examples[\"prompt\"],\n            padding=\"max_length\",\n            max_length=512,\n            truncation=True\n        )\n\n    tokenized = dataset.map(tokenize_fn, batched=True, remove_columns=[\"prompt\"])\n    tokenized.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n    return tokenized\n\ndef run_inference(tokenized_dataset, model, tokenizer, batch_size=8):\n    dataloader = DataLoader(tokenized_dataset, batch_size=batch_size, shuffle=False)\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n    model.eval()\n\n    predictions = []\n\n    for batch in dataloader:\n        input_ids = batch[\"input_ids\"].to(device)\n        attention_mask = batch[\"attention_mask\"].to(device)\n\n        with torch.no_grad():\n            outputs = model.generate(\n                input_ids=input_ids,\n                attention_mask=attention_mask,\n                max_length=128,\n                num_beams=4,\n                early_stopping=True\n            )\n\n        decoded_preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n        predictions.extend(decoded_preds)\n\n    return predictions\n\ndef save_predictions(df, predictions, output_path, id_column=\"ID\"):\n    df[\"predicted_improvement_solutions\"] = predictions\n\n    if id_column in df.columns:\n        output_df = df[[id_column, \"predicted_improvement_solutions\"]]\n        output_df.to_csv(output_path, index=False)\n    else:\n        raise KeyError(f\"The column '{id_column}' does not exist in the dataframe.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:49.512863Z","iopub.status.idle":"2025-05-08T16:41:49.514559Z","shell.execute_reply.started":"2025-05-08T16:41:49.513261Z","shell.execute_reply":"2025-05-08T16:41:49.513276Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Main Execution\nif __name__ == \"__main__\":\n    # Set device\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    \n    # Load and preprocess data\n    train_path = \"/kaggle/input/indabax-tunisia-2025-anomaly-solver-challenge-2/Train.csv\"\n    df = load_and_preprocess_data(train_path)\n    df = create_prompts(df)\n    \n    # Split data\n    train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n    \n    # Initialize model and tokenizer\n    model, tokenizer, optimizer,lr_scheduler = initialize_model_and_optimizer()\n    model.to(device)\n    \n    # Prepare datasets\n    tokenized_train, tokenized_val = prepare_datasets(train_df, val_df, tokenizer)\n    \n    # Create dataloaders\n    train_dataloader = DataLoader(tokenized_train, batch_size=BATCH_SIZE, shuffle=True)\n    val_dataloader = DataLoader(tokenized_val, batch_size=BATCH_SIZE)\n    \n    # Train model\n    model = train_model(model, train_dataloader, val_dataloader, optimizer,lr_scheduler, tokenizer, device)\n    \n    # Save model\n    model.save_pretrained(\"bart_solver\")\n    tokenizer.save_pretrained(\"bart_solver\")\n    \n    #Trying the model on the test dataset\n    # Paths\n    kpi_path = \"/kaggle/input/indabax-tunisia-2025-anomaly-solver-challenge-2/Test.csv\"\n    predicted_label_path = \"/kaggle/input/indaba-test/submission_indabax.csv\"\n    output_path = \"test_predictions.csv\"\n\n    # Process\n    df = prepare_test_dataframe(kpi_path, predicted_label_path)\n    df = create_prompt_column(df)\n    tokenized = tokenize_dataset(df, tokenizer)\n    predictions = run_inference(tokenized, model, tokenizer)\n    save_predictions(df, predictions, output_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-08T16:41:49.515446Z","iopub.status.idle":"2025-05-08T16:41:49.515851Z","shell.execute_reply.started":"2025-05-08T16:41:49.515583Z","shell.execute_reply":"2025-05-08T16:41:49.515595Z"}},"outputs":[],"execution_count":null}]}